# -*- coding: utf-8 -*-
"""
PrÃ©traitement des donnÃ©es - Satisfaction Passagers AÃ©riens
AdaptÃ© pour pipeline DVC avec gestion des batches
"""

import pandas as pd
import numpy as np
import yaml
import os
import joblib
from pathlib import Path
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')


def load_params():
    """Charge les paramÃ¨tres depuis params.yaml"""
    try:
        with open('params.yaml', 'r') as f:
            params = yaml.safe_load(f)
        return params.get('preprocess', {})
    except FileNotFoundError:
        print("âš ï¸  params.yaml non trouvÃ©, utilisation des paramÃ¨tres par dÃ©faut")
        return {}


def load_batches(train_dir='data/raw/train', test_dir='data/raw/test'):
    """Charge tous les batches de donnÃ©es"""
    print("="*80)
    print("âœˆï¸ PRÃ‰TRAITEMENT - SATISFACTION PASSAGERS AÃ‰RIENS")
    print("="*80)
    
    print("\n" + "="*80)
    print("ğŸ“ SECTION 1: CHARGEMENT DES DONNÃ‰ES")
    print("="*80)
    
    train_batches = []
    
    # Charger tous les fichiers batch_*.csv du dossier train
    batch_files = sorted(Path(train_dir).glob('batch_*.csv'))
    
    if not batch_files:
        raise FileNotFoundError(f"Aucun fichier batch trouvÃ© dans {train_dir}")
    
    for batch_file in batch_files:
        df = pd.read_csv(batch_file)
        train_batches.append(df)
        print(f"âœ… {batch_file.name}: {len(df):,} lignes")
    
    # ConcatÃ©ner tous les batches
    train_df = pd.concat(train_batches, ignore_index=True)
    
    # Charger les donnÃ©es de test
    test_file = Path(test_dir) / 'batch_test.csv'
    test_df = pd.read_csv(test_file)
    
    print(f"\nğŸ“Š Total Train: {train_df.shape[0]:,} lignes Ã— {train_df.shape[1]} colonnes")
    print(f"ğŸ“Š Test: {test_df.shape[0]:,} lignes Ã— {test_df.shape[1]} colonnes")
    
    return train_df, test_df


def drop_useless_columns(train_df, test_df):
    """Supprime les colonnes inutiles"""
    print("\n" + "="*80)
    print("ğŸ¯ SECTION 2: SUPPRESSION DES COLONNES INUTILES")
    print("="*80)
    
    cols_to_drop = ['id']
    
    train_df = train_df.drop(columns=cols_to_drop, errors='ignore')
    test_df = test_df.drop(columns=cols_to_drop, errors='ignore')
    
    print(f"âœ… Colonnes supprimÃ©es: {cols_to_drop}")
    print(f"ğŸ“Š Dimensions aprÃ¨s suppression:")
    print(f"   Train: {train_df.shape[0]:,} lignes Ã— {train_df.shape[1]} colonnes")
    print(f"   Test: {test_df.shape[0]:,} lignes Ã— {test_df.shape[1]} colonnes")
    
    return train_df, test_df


def handle_missing_values(train_df, test_df):
    """GÃ¨re les valeurs manquantes"""
    print("\n" + "="*80)
    print("ğŸ”§ SECTION 3: GESTION DES VALEURS MANQUANTES")
    print("="*80)
    
    missing_train = train_df.isnull().sum()
    missing_test = test_df.isnull().sum()
    
    missing_cols_train = missing_train[missing_train > 0].index.tolist()
    missing_cols_test = missing_test[missing_test > 0].index.tolist()
    
    if missing_cols_train or missing_cols_test:
        print("âš ï¸  Colonnes avec valeurs manquantes:")
        
        for df_name, df_missing, missing_cols, df in [
            ("Train", missing_train, missing_cols_train, train_df),
            ("Test", missing_test, missing_cols_test, test_df)
        ]:
            if missing_cols:
                print(f"\nğŸ”¸ {df_name}:")
                for col in missing_cols:
                    missing_pct = (df_missing[col] / len(df)) * 100
                    print(f"   - {col}: {df_missing[col]} valeurs ({missing_pct:.2f}%)")
        
        print("\nğŸ¯ StratÃ©gie d'imputation:")
        
        # 1. Colonnes catÃ©gorielles: Mode
        categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
        cat_missing = [col for col in categorical_cols if col in missing_cols_train or col in missing_cols_test]
        
        if cat_missing:
            print("   ğŸ”¸ CatÃ©gorielles: Imputation par mode")
            for col in cat_missing:
                if col in train_df.columns:
                    mode_val = train_df[col].mode()[0]
                    train_df[col] = train_df[col].fillna(mode_val)
                if col in test_df.columns:
                    mode_val = test_df[col].mode()[0] if not test_df[col].mode().empty else train_df[col].mode()[0]
                    test_df[col] = test_df[col].fillna(mode_val)
        
        # 2. Colonnes numÃ©riques: MÃ©diane
        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
        num_missing = [col for col in numeric_cols if col in missing_cols_train or col in missing_cols_test]
        
        if num_missing:
            print("   ğŸ”¸ NumÃ©riques: Imputation par mÃ©diane")
            for col in num_missing:
                if col in train_df.columns:
                    median_val = train_df[col].median()
                    train_df[col] = train_df[col].fillna(median_val)
                if col in test_df.columns:
                    median_val = test_df[col].median() if not test_df[col].isnull().all() else train_df[col].median()
                    test_df[col] = test_df[col].fillna(median_val)
        
        print("\nâœ… AprÃ¨s imputation:")
        print(f"   Train - Valeurs manquantes: {train_df.isnull().sum().sum()}")
        print(f"   Test - Valeurs manquantes: {test_df.isnull().sum().sum()}")
    else:
        print("âœ… Aucune valeur manquante dÃ©tectÃ©e!")
    
    return train_df, test_df


def encode_categorical_features(train_df, test_df):
    """Encode les variables catÃ©gorielles"""
    print("\n" + "="*80)
    print("ğŸ·ï¸ SECTION 4: ENCODAGE DES VARIABLES CATÃ‰GORIELLES")
    print("="*80)
    
    categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
    target_col = 'satisfaction'
    
    if target_col in categorical_cols:
        categorical_cols.remove(target_col)
    
    print(f"ğŸ” Variables catÃ©gorielles Ã  encoder: {categorical_cols}")
    
    encoders = {}
    
    for col in categorical_cols:
        print(f"\nğŸ”¸ Encodage de '{col}':")
        
        if train_df[col].nunique() == 2:
            print(f"   âš¡ Encodage binaire (Label Encoding)")
            le = LabelEncoder()
            
            train_df[col] = le.fit_transform(train_df[col])
            
            test_categories = set(test_df[col].unique())
            train_categories = set(le.classes_)
            
            if not test_categories.issubset(train_categories):
                print(f"   âš ï¸  CatÃ©gories inconnues dans test: {test_categories - train_categories}")
                most_frequent = train_df[col].mode()[0]
                test_df[col] = test_df[col].apply(lambda x: x if x in le.classes_ else le.classes_[most_frequent])
            
            test_df[col] = le.transform(test_df[col])
            encoders[col] = le
            
            print(f"   ğŸ“Š Mapping: {dict(zip(le.classes_, range(len(le.classes_))))}")
        
        else:
            print(f"   ğŸ¯ One-Hot Encoding ({train_df[col].nunique()} catÃ©gories)")
            
            dummies_train = pd.get_dummies(train_df[col], prefix=col, drop_first=True)
            dummies_test = pd.get_dummies(test_df[col], prefix=col, drop_first=True)
            
            missing_cols = set(dummies_train.columns) - set(dummies_test.columns)
            for c in missing_cols:
                dummies_test[c] = 0
            
            dummies_test = dummies_test[dummies_train.columns]
            
            train_df = pd.concat([train_df.drop(columns=[col]), dummies_train], axis=1)
            test_df = pd.concat([test_df.drop(columns=[col]), dummies_test], axis=1)
            
            print(f"   ğŸ“Š Colonnes crÃ©Ã©es: {list(dummies_train.columns)}")
    
    print(f"\nâœ… Encodage terminÃ©!")
    print(f"ğŸ“Š Nouvelles dimensions:")
    print(f"   Train: {train_df.shape[0]:,} lignes Ã— {train_df.shape[1]} colonnes")
    print(f"   Test: {test_df.shape[0]:,} lignes Ã— {test_df.shape[1]} colonnes")
    
    return train_df, test_df, encoders


def encode_target(train_df, test_df):
    """Encode la variable cible"""
    print("\n" + "="*80)
    print("ğŸ¯ SECTION 5: ENCODAGE DE LA VARIABLE CIBLE")
    print("="*80)
    
    target_col = 'satisfaction'
    
    print("ğŸ”¸ Encodage de la variable cible 'satisfaction'")
    target_encoder = LabelEncoder()
    train_df[target_col] = target_encoder.fit_transform(train_df[target_col])
    
    if target_col in test_df.columns:
        test_df[target_col] = target_encoder.transform(test_df[target_col])
    
    print(f"ğŸ“Š Mapping cible: {dict(zip(target_encoder.classes_, target_encoder.transform(target_encoder.classes_)))}")
    print(f"   - 0: {target_encoder.classes_[0]}")
    print(f"   - 1: {target_encoder.classes_[1]}")
    
    return train_df, test_df, target_encoder


def handle_outliers(train_df, test_df):
    """GÃ¨re les outliers"""
    print("\n" + "="*80)
    print("ğŸ“ˆ SECTION 6: GESTION DES OUTLIERS")
    print("="*80)
    
    target_col = 'satisfaction'
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    if target_col in numeric_cols:
        numeric_cols.remove(target_col)
    
    skewed_cols = ['Departure Delay in Minutes', 'Arrival Delay in Minutes']
    
    print("ğŸ” Gestion des outliers via winsorization ou transformation:")
    
    for col in numeric_cols:
        if col in skewed_cols:
            print(f"ğŸ”¸ {col}: Forte asymÃ©trie dÃ©tectÃ©e â†’ Transformation log")
            
            train_df[f'{col}_log'] = np.log1p(train_df[col])
            test_df[f'{col}_log'] = np.log1p(test_df[col])
            
            train_df = train_df.drop(columns=[col])
            test_df = test_df.drop(columns=[col])
        else:
            Q1 = train_df[col].quantile(0.01)
            Q3 = train_df[col].quantile(0.99)
            
            outliers_count = ((train_df[col] < Q1) | (train_df[col] > Q3)).sum()
            if outliers_count > 0 and outliers_count < len(train_df) * 0.05:
                print(f"ğŸ”¸ {col}: {outliers_count} outliers â†’ Winsorization (1%-99%)")
                
                train_df[col] = train_df[col].clip(Q1, Q3)
                test_df[col] = test_df[col].clip(Q1, Q3)
    
    print("\nâœ… Gestion des outliers terminÃ©e!")
    
    return train_df, test_df


def scale_features(train_df, test_df):
    """Normalise les features"""
    print("\n" + "="*80)
    print("âš–ï¸ SECTION 7: NORMALISATION/STANDARDISATION")
    print("="*80)
    
    target_col = 'satisfaction'
    
    X_train = train_df.drop(columns=[target_col])
    y_train = train_df[target_col]
    
    X_test = test_df.drop(columns=[target_col], errors='ignore')
    y_test = test_df[target_col] if target_col in test_df.columns else None
    
    print(f"ğŸ” {len(X_train.columns)} features Ã  normaliser")
    
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
    
    print("âœ… Normalisation avec RobustScaler terminÃ©e!")
    
    return X_train_scaled_df, X_test_scaled_df, y_train, y_test, scaler


def select_features(X_train, X_test, y_train, k=20):
    """SÃ©lectionne les meilleures features"""
    print("\n" + "="*80)
    print("ğŸ” SECTION 8: SÃ‰LECTION DE FEATURES")
    print("="*80)
    
    # 1. Variance threshold
    print("1ï¸âƒ£  Suppression des features Ã  variance nulle:")
    selector_variance = VarianceThreshold(threshold=0.01)
    X_train_var = selector_variance.fit_transform(X_train)
    X_test_var = selector_variance.transform(X_test)
    
    selected_features = X_train.columns[selector_variance.get_support()]
    print(f"   âœ… {len(selected_features)} features conservÃ©es sur {X_train.shape[1]}")
    
    # 2. SelectKBest
    print("\n2ï¸âƒ£  SÃ©lection basÃ©e sur ANOVA F-value:")
    k = min(k, len(selected_features))
    selector_kbest = SelectKBest(score_func=f_classif, k=k)
    X_train_selected = selector_kbest.fit_transform(X_train[selected_features], y_train)
    X_test_selected = selector_kbest.transform(X_test[selected_features])
    
    selected_features_kbest = selected_features[selector_kbest.get_support()]
    
    print(f"   âœ… Top {k} features sÃ©lectionnÃ©es:")
    scores = selector_kbest.scores_
    indices = np.argsort(scores)[::-1]
    
    for i in range(min(10, len(scores))):
        idx = indices[i]
        print(f"      {i+1:2d}. {selected_features[idx]:30} : {scores[idx]:.2f}")
    
    X_train_final = pd.DataFrame(X_train_selected, columns=selected_features_kbest, index=X_train.index)
    X_test_final = pd.DataFrame(X_test_selected, columns=selected_features_kbest, index=X_test.index)
    
    print(f"\nğŸ“Š Dimensions finales:")
    print(f"   X_train: {X_train_final.shape}")
    print(f"   X_test: {X_test_final.shape}")
    
    return X_train_final, X_test_final, selector_variance, selector_kbest, selected_features_kbest


def save_processed_data(X_train, X_test, y_train, y_test, scaler, variance_selector, 
                       kbest_selector, target_encoder, encoders, selected_features,
                       output_dir='data/processed'):
    """Sauvegarde les donnÃ©es prÃ©traitÃ©es"""
    print("\n" + "="*80)
    print("ğŸ’¾ SECTION 9: SAUVEGARDE DES DONNÃ‰ES PRÃ‰TRAITÃ‰ES")
    print("="*80)
    
    os.makedirs(output_dir, exist_ok=True)
    
    X_train.to_csv(f'{output_dir}/X_train_processed.csv', index=False)
    X_test.to_csv(f'{output_dir}/X_test_processed.csv', index=False)
    
    y_train.to_csv(f'{output_dir}/y_train_processed.csv', index=False)
    if y_test is not None:
        y_test.to_csv(f'{output_dir}/y_test_processed.csv', index=False)
    
    preprocessing_objects = {
        'scaler': scaler,
        'variance_selector': variance_selector,
        'kbest_selector': kbest_selector,
        'target_encoder': target_encoder,
        'encoders': encoders,
        'selected_features': selected_features.tolist()
    }
    
    joblib.dump(preprocessing_objects, f'{output_dir}/preprocessing_objects.pkl')
    
    print("âœ… DonnÃ©es sauvegardÃ©es avec succÃ¨s!")
    print(f"ğŸ“ Dossier: {output_dir}")
    print(f"ğŸ“„ Fichiers gÃ©nÃ©rÃ©s:")
    print(f"   - X_train_processed.csv")
    print(f"   - X_test_processed.csv")
    print(f"   - y_train_processed.csv")
    if y_test is not None:
        print(f"   - y_test_processed.csv")
    print(f"   - preprocessing_objects.pkl")


def print_summary(initial_shape, final_shape, X_train, X_test):
    """Affiche le rÃ©sumÃ© du prÃ©traitement"""
    print("\n" + "="*80)
    print("ğŸ“Š SECTION 10: RÃ‰SUMÃ‰ DU PRÃ‰TRAITEMENT")
    print("="*80)
    
    print("ğŸ¯ Ã‰TAPES EFFECTUÃ‰ES:")
    print("""
1. âœ… Chargement des donnÃ©es (batches)
2. âœ… Suppression des colonnes inutiles (id)
3. âœ… Gestion des valeurs manquantes (imputation mÃ©diane/mode)
4. âœ… Encodage des variables catÃ©gorielles
5. âœ… Encodage de la variable cible (satisfaction â†’ 0/1)
6. âœ… Gestion des outliers
7. âœ… Normalisation avec RobustScaler
8. âœ… SÃ©lection de features
9. âœ… Sauvegarde des donnÃ©es prÃ©traitÃ©es
""")
    
    print("ğŸ“ˆ STATISTIQUES FINALES:")
    print(f"   - Nombre de features initial: {initial_shape[1] - 1}")
    print(f"   - Nombre de features final: {final_shape[1]}")
    reduction = ((initial_shape[1] - 1 - final_shape[1]) / (initial_shape[1] - 1) * 100)
    print(f"   - RÃ©duction: {reduction:.1f}%")
    print(f"   - Taille Ã©chantillon train: {X_train.shape[0]:,}")
    print(f"   - Taille Ã©chantillon test: {X_test.shape[0]:,}")
    
    print("\nğŸ” APERÃ‡U DES DONNÃ‰ES PRÃ‰TRAITÃ‰ES (X_train):")
    print(X_train.head())
    
    print("\n" + "="*80)
    print("âœ… PRÃ‰TRAITEMENT TERMINÃ‰ - PRÃŠT POUR LA MODÃ‰LISATION!")
    print("="*80)


def main():
    """Pipeline principal de preprocessing"""
    # Charger les paramÃ¨tres
    params = load_params()
    
    train_dir = params.get('train_dir', 'data/raw/train')
    test_dir = params.get('test_dir', 'data/raw/test')
    output_dir = params.get('output_dir', 'data/processed')
    k_features = params.get('k_best_features', 20)
    
    # 1. Charger les donnÃ©es
    train_df, test_df = load_batches(train_dir, test_dir)
    initial_shape = train_df.shape
    
    # 2. Supprimer colonnes inutiles
    train_df, test_df = drop_useless_columns(train_df, test_df)
    
    # 3. GÃ©rer valeurs manquantes
    train_df, test_df = handle_missing_values(train_df, test_df)
    
    # 4. Encoder variables catÃ©gorielles
    train_df, test_df, encoders = encode_categorical_features(train_df, test_df)
    
    # 5. Encoder variable cible
    train_df, test_df, target_encoder = encode_target(train_df, test_df)
    
    # 6. GÃ©rer outliers
    train_df, test_df = handle_outliers(train_df, test_df)
    
    # 7. Normaliser
    X_train, X_test, y_train, y_test, scaler = scale_features(train_df, test_df)
    
    # 8. SÃ©lectionner features
    X_train_final, X_test_final, variance_selector, kbest_selector, selected_features = \
        select_features(X_train, X_test, y_train, k=k_features)
    
    # 9. Sauvegarder
    save_processed_data(X_train_final, X_test_final, y_train, y_test, 
                       scaler, variance_selector, kbest_selector, 
                       target_encoder, encoders, selected_features, output_dir)
    
    # 10. RÃ©sumÃ©
    print_summary(initial_shape, X_train_final.shape, X_train_final, X_test_final)


if __name__ == "__main__":
    main()